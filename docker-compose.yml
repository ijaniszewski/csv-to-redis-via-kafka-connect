version: "3.9"

services:
    kafka:
        hostname: kafka
        image: confluentinc/cp-enterprise-kafka:5.0.0
        depends_on:
          - zookeeper
        environment:
          KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
          KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://:9092"
          KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
          CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: localhost:9092
          CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
          CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
        ports:
          - "9092:9092"
    schema-registry:
        image: confluentinc/cp-schema-registry:5.0.0
        environment:
          SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: "zookeeper:2181"
          SCHEMA_REGISTRY_HOST_NAME: schema-registry
        depends_on:
          - zookeeper
          - kafka
        ports:
          - '8081:8081'
    zookeeper:
        image: confluentinc/cp-zookeeper:5.0.0
        environment:
          ZOOKEEPER_CLIENT_PORT: "2181"
          ZOOKEEPER_TICK_TIME: "2000"
    connect:
        hostname: connect
        build:
            context: ./kafka_connect/
            dockerfile: ./Dockerfile
        depends_on:
            - schema-registry
            - kafka
        ports:
        - '8083:8083'
        environment:
            CONNECT_BOOTSTRAP_SERVERS: "kafka:9092"
            CONNECT_REST_ADVERTISED_HOST_NAME: target-connect
            CONNECT_PRODUCER_COMPRESSION_TYPE: lz4
            CONNECT_GROUP_ID: connect
            CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
            CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
            CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
            CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
            CONNECT_CONFIG_STORAGE_TOPIC: connect_config
            CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
            CONNECT_OFFSET_STORAGE_TOPIC: connect_offset
            CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
            CONNECT_STATUS_STORAGE_TOPIC: connect_status
            CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
            CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
            CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
            CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE: 'false'
            CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE: 'false'
            CONNECT_LOG4J_LOGGERS: 'org.reflections=ERROR'
            CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/redis-sink,/usr/share/confluent-hub-components"
            # External secrets config
            # See https://docs.confluent.io/current/connect/security.html#externalizing-secrets
            CONNECT_CONFIG_PROVIDERS: 'file'
            CONNECT_CONFIG_PROVIDERS_FILE_CLASS: 'org.apache.kafka.common.config.provider.FileConfigProvider'
        volumes:
            - './csv_data/:/csv_data'
            - './connectors/:/connectors'

    redis:
        image: redis:4.0.5-alpine
        command: ["redis-server", "--appendonly", "yes"]
        hostname: redis
        volumes:
            - redis_data:/data

    python:
      build:
          context: ./python_docker/
          dockerfile: ./Dockerfile
      tty: true
    
volumes:
    redis_data:
